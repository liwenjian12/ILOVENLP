# 什么是卷积 <br>
公式如图：<br>
![]https://github.com/liwenjian12/ILOVENLP/blob/master/pictures/590093-20160825232332726-1929898901.jpg.png) <br>
在LeNet5中，卷积其实就是要卷积的区域和卷积核的点乘和，加上偏置之后的激活输出。
CNN中哪些层是需要激活函数的？刚才我已经说了，在C层确实是需要激活函数的，那么在其他层需要吗？且看下面的图：

　　在这个图里主要展现的是卷积操作和池化操作，但是都没有体现f函数，即激活函数。其实，在卷积层和池化层，在最后都是需要加上偏置激活输出的。但是，有些神经网络在实现的时候，可能并不会去实现激活。比如，卷积操作直接将w和x的点积和作为输出，而池化操作直接使用1-max将top1的值输出，而不进行激活操作。要知道，sigmoid，tanh以及ReLU等激活函数，可以非常好的捕捉到非线性的特征。因此，在LeNet5中，卷积层和池化层都会进行偏置激活。全连接层F6和普通神经网络的隐层是一样的，也是最后要激活输出的，因此也需要激活函数。其实，CNN可以理解为，除了卷积操作和池化操作，其余的和NN没有区别，最后只是跟了一个多分类器。因此，CNN的训练也是基于BP算法，利用随机梯度下降（SGD）进行参数训练。

#CNN在自然语言处理中的应用
